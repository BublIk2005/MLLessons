{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315d1fa5",
   "metadata": {},
   "source": [
    "## Очистка текста"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fa47e0",
   "metadata": {},
   "source": [
    "Даны некоторые неструктурированные текстовые данные, и требуется выполнить их элементарную очистку"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5ba044",
   "metadata": {},
   "source": [
    "Большинство элементарных операций очистки текста соответствует элементарным операциям языка Python над строковыми значениями, в частности strip, replcae и split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e42b1254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang. By Aishwarya Henriette',\n",
       " 'Parking And Going. By Karl Gautier',\n",
       " 'Today Is The night. By Jarek Prakash']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создать текст\n",
    "text_data = [\"  Interrobang. By Aishwarya Henriette   \",\n",
    "            \"Parking And Going. By Karl Gautier\",\n",
    "            \"    Today Is The night. By Jarek Prakash    \"]\n",
    "\n",
    "# удалить пробелы\n",
    "strip_whitespace = [string.strip() for string in text_data]\n",
    "\n",
    "strip_whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20316849",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Interrobang By Aishwarya Henriette',\n",
       " 'Parking And Going By Karl Gautier',\n",
       " 'Today Is The night By Jarek Prakash']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# удалить точки\n",
    "remove_periods = [string.replace(\".\",\"\") for string in strip_whitespace]\n",
    "\n",
    "remove_periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b58af76",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['INTERROBANG BY AISHWARYA HENRIETTE',\n",
       " 'PARKING AND GOING BY KARL GAUTIER',\n",
       " 'TODAY IS THE NIGHT BY JAREK PRAKASH']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# собственная функция преобразования\n",
    "def capitalizer(string: str) -> str:\n",
    "    return string.upper()\n",
    "\n",
    "\n",
    "[capitalizer(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ed8a31b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['XXXXXXXXXXX XX XXXXXXXXX XXXXXXXXX',\n",
       " 'XXXXXXX XXX XXXXX XX XXXX XXXXXXX',\n",
       " 'XXXXX XX XXX XXXXX XX XXXXX XXXXXXX']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# использование регулярных выражений\n",
    "import re\n",
    "\n",
    "# создадим функцию\n",
    "replace_latters_with_X = lambda string: re.sub(r\"[a-zA-Z]\", \"X\", string)\n",
    "\n",
    "[replace_latters_with_X(string) for string in remove_periods]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e4882a",
   "metadata": {},
   "source": [
    "## Разбор и очистка разметки HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e7e3636",
   "metadata": {},
   "source": [
    "Даны текстовые дпнные с элементами HTML, и требуется извлечь только текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8892e4bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html><body><div class=\"full_name\"><span style=\"font-weight:bold\">Masego</span> Azra</div>\n",
       "</body></html>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для решения используется функционал библиотеки Beautiful Soup\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Создаем немного кода с разметкой HTML\n",
    "html = \"\"\" \n",
    "<div class = 'full_name'><span style = 'font-weight:bold'>Masego</span> Azra</div>\n",
    "\"\"\"\n",
    "# Выполнить разбор HTML\n",
    "soup = BeautifulSoup(html, \"lxml\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0106d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Masego Azra'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Найти элемент div с классом \"full_name\"\n",
    "soup.find(\"div\", {\"class\" : \"full_name\"}).text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a819d2f",
   "metadata": {},
   "source": [
    "## Удаление знаков препинания"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da63fe",
   "metadata": {},
   "source": [
    "Дан признак в виде текстовых данных, и требуется удалить знаки препинания"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8aa9d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hi I Love This Song', '10000 Agree LoveIt', 'Right']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# определить функцию, которая использует метод translate со словарем знаков препинания\n",
    "import unicodedata\n",
    "import sys\n",
    "\n",
    "text_data = ['Hi!!!! I. Love. This. Song.....',\n",
    "            '10000% Agree!!!!! #LoveIt',\n",
    "            'Right?!?!?!']\n",
    "\n",
    "# Создать сдоварь знаков препинания\n",
    "punctuation = dict.fromkeys(i for i in range(sys.maxunicode)\n",
    "                           if unicodedata.category(chr(i)).startswith('P'))\n",
    "\n",
    "# Удалить все знаки препинания\n",
    "[string.translate(punctuation) for string in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ee21c02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Po'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unicodedata.category(\"!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bad84c5",
   "metadata": {},
   "source": [
    "## Лексемизация текста\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e110f7d8",
   "metadata": {},
   "source": [
    "Дан текст, и требуется разбить его на отдельные слова\n",
    "\n",
    "Для решения будет использоваться комплект естественно-языковых инструментов NLTK (Natural Language Toolkit for Python)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5400f4ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mikhail/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import ssl\n",
    "\n",
    "try:\n",
    "    _create_unverified_https_context = ssl._create_unverified_context\n",
    "except AttributeError:\n",
    "    pass\n",
    "else:\n",
    "    ssl._create_default_https_context = _create_unverified_https_context\n",
    "\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fca26d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Сегодняшняя', 'наука', '-', 'это', 'технологии', 'завтрашнего', 'дня']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = \"Сегодняшняя наука - это технологии завтрашнего дня\"\n",
    "\n",
    "# Лексемизировать на слова\n",
    "word_tokenize(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "630c70f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Сегодняшняя наука - это технологии завтрашнего дня.',\n",
       " 'Завтра начинается сегодня']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Лексемизировать текст на предложения\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "string = \"\"\"Сегодняшняя наука - это технологии завтрашнего дня. Завтра начинается сегодня\"\"\"\n",
    "\n",
    "sent_tokenize(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2cea96",
   "metadata": {},
   "source": [
    "## Удаление стоп слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf3b36d",
   "metadata": {},
   "source": [
    "Имеются лексемизированные текстовые данные, из которых требуется удалить чрезвычайно общеупотребительные слова, несущие в себе мало информации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03dcfc0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'going', 'to', 'go', 'to', 'the', 'store', 'and', 'park']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Скачать набор стоп слов\n",
    "#import nltk\n",
    "#nltk.download('stopwords')\n",
    "\n",
    "# Создать лексемы слов\n",
    "string = 'i am going to go to the store and park'\n",
    "\n",
    "tokenized_words = word_tokenize(string)\n",
    "tokenized_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f528d001",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузить стоп-слова\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4534a315",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['going', 'go', 'store', 'park']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Удалить стоп слова\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "04e597e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['пошел', 'пиццерию', 'покушать', 'пиццы', 'парк']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для русских слов\n",
    "stop_words=stopwords.words('russian')\n",
    "\n",
    "string = 'я бы пошел в пиццерию покушать пиццы и потом в парк'\n",
    "\n",
    "tokenized_words = word_tokenize(string)\n",
    "\n",
    "[word for word in tokenized_words if word not in stop_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bde313",
   "metadata": {},
   "source": [
    "## Выделение основ слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae0dcbc",
   "metadata": {},
   "source": [
    "Даны лексемизированные слова, которые требуется преобразовать в их коренвые формы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4caa2efe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'am', 'humbl', 'by', 'thi', 'tradit', 'meet']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_words = word_tokenize('i am humbled by this traditional meeting')\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "[porter.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6222baf6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['рыбак', 'рыбак', 'рыбач', 'до', 'рыбк', 'снежн', 'днем', 'снег']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Для русских слов используется стеммер Snowball\n",
    "\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "tokenized_words = word_tokenize('рыбак рыбака рыбачил до рыбки снежным днем снега')\n",
    "\n",
    "snowball = SnowballStemmer('russian')\n",
    "\n",
    "[snowball.stem(word) for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a07c1",
   "metadata": {},
   "source": [
    "## Лемматизфция слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aacf0a2",
   "metadata": {},
   "source": [
    "Даны лексемизированные слова, которые требуется собрать в синономические ряды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a7127926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go', 'go', 'go', 'be', 'be', 'be', 'be', 'be']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_words = word_tokenize('go went gone am are is was were')\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "[lemmatizer.lemmatize(word, pos = 'v') for word in tokenized_words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf09c85",
   "metadata": {},
   "source": [
    "## Разметка слов на части речи"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862d3a02",
   "metadata": {},
   "source": [
    "Даны текстовые данные, и требуется пометить каждое слово или символ своей частью речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2d4a46c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Chris', 'NNP'), ('loved', 'VBD'), ('outdoor', 'RP'), ('running', 'VBG')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk import word_tokenize\n",
    "\n",
    "text_data = \"Chris loved outdoor running\"\n",
    "\n",
    "# Использовать предварительно натренированный разметчик частей речи\n",
    "text_tagged = pos_tag(word_tokenize(text_data))\n",
    "\n",
    "text_tagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876e38ba",
   "metadata": {},
   "source": [
    "Результат - список кортежей со словом и тегом части речи.\n",
    "\n",
    "Метки можно использовать для поиска слов определенной части речи"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2c8f1639",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chris']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[word for word, tag in text_tagged if tag in ['NN','NNS', 'NNP','NNPS']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ddde3a5",
   "metadata": {},
   "source": [
    "Более реалистичной является ситуация, когда есть данные, где каждое наблюдение содержит твит, и мы хотим преобразовать эти предложения в признаки отдельных частей речи (например признак с 1 если присутствует собственное существительное, и 0 в противном случае) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f15a4e5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 0, 1, 0, 1, 1, 1, 0],\n",
       "       [0, 1, 0, 1, 1, 0, 0, 0, 0, 1],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 0, 0, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "\n",
    "tweets = [\"I am eating, a burrito for breakfast\",\n",
    "         \"Political science is an amazing field\",\n",
    "         \"San Francisco is an awesome city\"]\n",
    "\n",
    "tagget_tweets = []\n",
    "\n",
    "for tweet in tweets:\n",
    "    tweet_tag = nltk.pos_tag(word_tokenize(tweet))\n",
    "    tagget_tweets.append([tag for word, tag in tweet_tag])\n",
    "    \n",
    "one_hot_multi = MultiLabelBinarizer()\n",
    "one_hot_multi.fit_transform(tagget_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "20dbf90d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([',', 'DT', 'IN', 'JJ', 'NN', 'NNP', 'PRP', 'VBG', 'VBP', 'VBZ'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_multi.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9a511f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3c/4b8b1st977j10m2kgbzyhk8r0000gn/T/ipykernel_42826/2705987682.py:18: DeprecationWarning: \n",
      "  Function evaluate() has been deprecated.  Use accuracy(gold)\n",
      "  instead.\n",
      "  trigram.evaluate(test)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8174734002697437"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# натренируем собственный разметчик \n",
    "from nltk.corpus import brown\n",
    "from nltk.tag import UnigramTagger, BigramTagger, TrigramTagger\n",
    "\n",
    "# Получим немного текста из Brown Corpus, разбитого на предложения\n",
    "sentences = brown.tagged_sents(categories='news')\n",
    "\n",
    "# Разобъем на 4000 предложений для тренировки и 623 для тестирования\n",
    "train = sentences[:4000]\n",
    "test = sentences[4000:]\n",
    "\n",
    "# Создать разметчик с откатом\n",
    "unigram = UnigramTagger(train)\n",
    "bigram = BigramTagger(train, backoff=unigram)\n",
    "trigram = TrigramTagger(train, backoff=bigram)\n",
    "\n",
    "# ОЦеним точность\n",
    "trigram.evaluate(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a55341a",
   "metadata": {},
   "source": [
    "## Кодирование текста в качестве мешка слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4be527b8",
   "metadata": {},
   "source": [
    "Даны текстовые данные, и требуется создать набор признаков, указывающих на количество вхождений определенного слова в текст наблюдений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7bdda6b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Используем класс CountVectorizer\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text_data = np.array(['Бразилия - моя любовь. Бразилия!',\n",
    "                     'Швеция - лучше',\n",
    "                     'Германия бьет обоих'])\n",
    "\n",
    "# Создать матрицу признаков на основе мешка слов\n",
    "count = CountVectorizer()\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "\n",
    "bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2e7bf83f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 0, 0, 0, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 1, 0, 0, 0, 1],\n",
       "       [0, 1, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5cf8c27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['бразилия', 'бьет', 'германия', 'лучше', 'любовь', 'моя', 'обоих',\n",
       "       'швеция'], dtype=object)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1cdda85c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>бразилия</th>\n",
       "      <th>бьет</th>\n",
       "      <th>германия</th>\n",
       "      <th>лучше</th>\n",
       "      <th>любовь</th>\n",
       "      <th>моя</th>\n",
       "      <th>обоих</th>\n",
       "      <th>швеция</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   бразилия  бьет  германия  лучше  любовь  моя  обоих  швеция\n",
       "0         2     0         0      0       1    1      0       0\n",
       "1         0     0         0      1       0    0      0       1\n",
       "2         0     1         1      0       0    0      1       0"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df =pd.DataFrame(data=bag_of_words.toarray(), columns=count.get_feature_names_out())\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc757b3",
   "metadata": {},
   "source": [
    "##  Взвешивание важности слов"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ba8340",
   "metadata": {},
   "source": [
    "Требуется мешок слов, но со словами, взвешенными по их важности для наблюдения"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b00d70fa",
   "metadata": {},
   "source": [
    "Сравним частоту слова в документе(твит, обзор видео, стенограмма выступления и тд) с частотой слова во всех других документах, используя статистическую меру словарной частоты - обратной документной частоты (tf-idf). Библиотека scikit-learn позволяет легко это сделать с помощью класса TfidVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0c602e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x8 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 8 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "text_data = np.array(['Бразилия - моя любовь. Бразилия!',\n",
    "                     'Швеция - лучше',\n",
    "                     'Германия бьет обоих'])\n",
    "\n",
    "# Создать матрицу признаков на основе меры tf-idf\n",
    "tfidf = TfidfVectorizer()\n",
    "feature_matrix = tfidf.fit_transform(text_data)\n",
    "\n",
    "# Показать матрицу признаков на основе меры tf-idf\n",
    "feature_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3e0d47b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81649658, 0.        , 0.        , 0.        , 0.40824829,\n",
       "        0.40824829, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.70710678, 0.        ,\n",
       "        0.        , 0.        , 0.70710678],\n",
       "       [0.        , 0.57735027, 0.57735027, 0.        , 0.        ,\n",
       "        0.        , 0.57735027, 0.        ]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_matrix.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b330c4c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'бразилия': 0,\n",
       " 'моя': 5,\n",
       " 'любовь': 4,\n",
       " 'швеция': 7,\n",
       " 'лучше': 3,\n",
       " 'германия': 2,\n",
       " 'бьет': 1,\n",
       " 'обоих': 6}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf.vocabulary_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
